---
layout: post
title:  "Recent Trends in AI Interpretability"
excerpt: "Recent developments in the field of mechanistic interpretability and the views of key players."
date:   2025-06-16
hide: false
categories: deep-learning llm visualisation interpretability safety
permalink: /2025/06/16/interpretability-trends.html/
---

## Background 
The passing few months saw a great renewed emphasis on safety and interpretability of AI systems as they becomes more capable following reinforcement post-training towards significantly better “reasoning” capabilities scaled at test time. The view of urgency and criticality for interpretability was initially introduced by Dario Amodei (CEO and cofounder of Anthropic) in his blog post The [Urgency of Interpretability](https://www.darioamodei.com/post/the-urgency-of-interpretability) April this year. This article received a great amount of attention and prompted reactions from some of the key players in the field including Dan Hendryks and Neel Nanda, both expressing their own views on the green field of mechanistic interpretability (AKA mech interp by the cool kids), AI safety and control. 

## Anthropic Doubles Down

The Urgency of Interpretability is a great read for anyone interested in Dario and Anthropic's stance on this topic. Anthropic has always focused on AI Safety through their alignment, interpretability and societal impact research divisions as well as work in [Constitutional AI](https://www.anthropic.com/news/constitutional-ai-harmlessness-from-ai-feedback) and [Responsible Scaling Policy](https://www.anthropic.com/news/anthropics-responsible-scaling-policy). Dario as their CEO, is one of the authors behind GPT-2 and GPT-3, which are the precursors to ChatGPT (essentially GPT-3 with further SFT and RLHF post-training steps). In the article, Dario introduced the field of mechanistic interpretability as an "accurate MRI that would fully reveal the inner workings of an AI model". This concept is not new and a similar analogy to microscopes has been drawn previously by another one of Anthropic's cofounders and world leading mech interp expert Chris Olah (who is also referenced a few times throughout Dario's article). 

Dario introduces the history of mech interp starting with Chris' work on earlier vision models, for example "wheel detector" neurons and the ["Jennifer Aniston" neuron](https://en.wikipedia.org/wiki/Grandmother_cell). Initial success has been possible in earlier vision models (which were no match in the number of parameters to today's language models) due to:
* Universality of the clean visual "concepts" (such as wheels) that need to be learned for the neural networks to do well in real-world image recognition / classification tasks.
* The redundancy of information in image information - nearby pixels are most likely to be very similar to each other.

The challenge of mech interp then came to language models as human texts as a medium of information is much denser than images. Within human language, it was no longer easy to find single neurons responsible for detecting car wheels or your grandmother as there are a lot more "concepts" and these "concepts" may now be distributed across many neurons. The phenomenon is known as superposition where the number of these "concepts" is much larger than the number of neurons. Dario then touches on the earlier work of Anthropic for language models through [Toy Models of Superposition](https://transformer-circuits.pub/2022/toy_model/index.html) (the technique for disentangling superimposed "concepts" sitting inside neurons of language models is called Sparse Autoencoders, or SAEs) through to more recent [Circuit Tracing](https://transformer-circuits.pub/2025/attribution-graphs/methods.html) (a further extension of SAEs to replace MLP layers completely with Sparsely activated expanded layers which has additional connections to later replacement layers) and [Biology of LLM](https://transformer-circuits.pub/2025/attribution-graphs/biology.html) techical posts. 

Anthropic also recently open sourced their [Circuit Tracing library](https://github.com/safety-research/circuit-tracer) (kudos to them) for everyone to view, extract and control for certain behaviours in a small language model, `Gemma-2-2b` and their proprietary Claude Haiku (limited examples only). The call to action for increased and timely effort on mech interp is urgent in the article as the stakes of AI are ever increasing especially in situations where explanations of underlying language model behaviours are critical. Dario encourages researchers not just to focus on shipping LLMs but to invest more in the young field of mech interp. Anthropic as a company is taking the lead and doubling down on the investment through interpretability startup [Goodfire](https://www.goodfire.ai/) and its own [mech interp research](https://www.anthropic.com/research#interpretability).

## The Case Against Mech Interp
In reaction to Dario's article, [The Misguided Quest for Mechanistic AI Interpretability](https://ai-frontiers.org/articles/the-misguided-quest-for-mechanistic-ai-interpretability) already spells out the other camp's view on mech interp. The author [Dan Hendrycks](https://en.wikipedia.org/wiki/Dan_Hendrycks) is a well known researcher working in the area of Machine Learning / AI Safety and Robustness. He is also a leading figure in creating several of the well known LLM benchmark datasets - MMLU, BIG Bench, MATH and more recently Humanity's Last Exam. The article opens with a reference to Google DeepMind's deprioritisation of Sparse Autoencoders and SAE-based technology (directly in line with Anthropic's research into SAE and circuit tracing technology) end of March this year. A key point of argument which I can see the benefits of would be a different approach championed by Dan here called representation engineering or activation hacking. This represents a top down approach where pairs of texts would be given to an LLM and their difference in activations captured (no finetuning involved). This "difference" vector can then be applied in more general use-cases to steer / control the behaviour of language models. This methodology is very much in contrast to mech interp that is a bottom up approach. I do concur with the overall idea to try different approaches but don't necessarily agree with a specific argument against mech interp. It was purported that because "people often fail to understand even their own decisions and actions", mech interp thus cannot be helpful. This is quite the opposite as there would be cases where people may not be able to understand their own actions but an MRI scan of the brain can provide further insights. 

## A More Balanced View
This last section will be going further into the details of the argument against mech interp. Behind both Dan's reference to Google DeepMind's [Deprioritising SAE Research Update](https://www.alignmentforum.org/posts/4uXCAJNuPKtKBsi28/sae-progress-update-2-draft) and [Interpretability Will Not Reliably Find Deceptive AI](https://www.alignmentforum.org/posts/PwnadG4BFjaER3MGf/interpretability-will-not-reliably-find-deceptive-ai) is another well known interpretability researcher Neel Nanda (though in the latter post Neel is expressing his own personal views). I tend to agree more about the conclusions of this article - that AI Safety is such a high stake field that a suite or battery of tools are needed at different levels of abstraction for testing if there are unexpected behaviours if we're to have a fair chance of success. No single approach should be hailed as a silver bullet to every problem (or no free lunch in the mantra of traditional machine learning). 

A perfect analogy would be detecting when people lie. We can start with asking them as is or design innovative verbal phrasings and look for clues. This could be followed by more sophisticated polygraphs or lie detectors then finally MRI scans of the brain. As for LLMs, the hand-wavy equivalent of this can range from:
* Blackbox simple prompting methods - ask the LLM to explain their thinking
* Blackbox Red Teaming methods - innovative ways to prompt models to induce them to behave unexpectedly / jailbreak them
* Supervised method such as Linear Probing - where the weights of LLMs will be frozen and a linear head is added to the last hidden state with supervised labels for the outcome / behaviour of interest to extract "concepts" from the activations (weights of the additional layer is updated through training)
* Supervised method such as Representation Engineering - for a top-down approach as advocated in Dan's article (no extra training)
* Unsupervised method such as Mechanistic Interpretability - for a bottom-up mechanistic understanding as advocated in Dario's article (heavy extra training required for obtaining the SAEs and Cross-layer Transcoders).

## Final Thing
If you've read this far, you would have a deep interest and understand that this article is just the tip of the iceberg on the topic (or simply scrolled to the bottom :P). I'll write a follow-up technical blog on what I've explored for mechanistic interpretability (potentially also in comparison to representation engineering). Till then, keep learning!